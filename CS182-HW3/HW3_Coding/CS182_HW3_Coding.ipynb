{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2401ead",
   "metadata": {},
   "source": [
    "# CS182 HW3 Coding [40 points]\n",
    "\n",
    "In this coding homework, you will be required to complete several models for binary classification and try to find the inplicit relationship of them by yourself. \n",
    "\n",
    "**Good luck!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1700370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from scipy import special\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce572b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('data/X_train.txt')\n",
    "X_val = np.loadtxt('data/X_val.txt')\n",
    "X_test = np.loadtxt('data/X_test.txt')\n",
    "y_train = np.loadtxt('data/y_train.txt')\n",
    "y_val = np.loadtxt('data/y_val.txt')\n",
    "y_test = np.loadtxt('data/y_test.txt')\n",
    "\n",
    "w = np.loadtxt('data/w.txt')\n",
    "w0 = np.loadtxt('data/w0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7697c3",
   "metadata": {},
   "source": [
    "## (a) Simple Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21414ceb",
   "metadata": {},
   "source": [
    "(1) Activation functions and loss functions are important parts of each neural network, and there are multiple ways of calculating them. \n",
    "\n",
    " **[3 points]** In this question, we ask you to implement the **sigmoid function** and **binary cross entroy loss function** serving for the binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08486cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def BCEloss(y_pred, y):\n",
    "    return - np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74b28f",
   "metadata": {},
   "source": [
    "(2) **[3 points]** In this question, we ask you to implement the **softmax function** and **cross entroy loss function** serving for the multiple classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0c07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y, y_pre):\n",
    "    return -np.mean(np.sum(y * np.log(y_pre), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629407c",
   "metadata": {},
   "source": [
    "(3) **[10 points]** Learning a simple perceptron with **batch GD** (using the given initializations $w^{init}$ and $w^{init}_{0}$) based on the training set ($X_{train}$, $y_{train}$): use the training set and the validation set to obtain a good learning rate (you can set the maximum for iterations to 50 and try different learning rate in [$10^{−4}$,  $10^{−8}$] ); output the learned model and evaluate its performance on the test set with the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d0247e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.0001\n",
      "Best accuracy on train: 0.97\n",
      "Best Model weights:\n",
      " [[-0.12671047]\n",
      " [-0.11962769]\n",
      " [-0.11669877]\n",
      " [-0.12621546]\n",
      " [-0.12178165]\n",
      " [-0.12109535]\n",
      " [-0.12320885]\n",
      " [-0.12065023]\n",
      " [-0.12631277]\n",
      " [-0.11899523]\n",
      " [-0.12389209]\n",
      " [-0.11669966]\n",
      " [-0.12400644]\n",
      " [-0.12341878]\n",
      " [-0.12122757]\n",
      " [-0.12278914]\n",
      " [-0.12500693]\n",
      " [-0.11854638]\n",
      " [-0.12232647]\n",
      " [-0.12065187]]\n",
      "Best Model bias: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# BGD Implementation\n",
    "max_iter = 50\n",
    "learning_rates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "\n",
    "train_accs = []\n",
    "train_w = []\n",
    "train_w0 = []\n",
    "best_lr = None\n",
    "best_acc = 0\n",
    "\n",
    "# compute the gradient of the loss function with respect to the weights\n",
    "def grad(x, y, w, w0):\n",
    "    y_pred = sigmoid(np.dot(x, w) + w0)\n",
    "    grad_w = np.dot(x.T, (y_pred - y)) * (1 / (2 * x.shape[1]))\n",
    "    return grad_w\n",
    "\n",
    "# iterate and update the weights\n",
    "def BatchGD(x, y, w, w0, lr, max_iter):\n",
    "    for i in range(max_iter):\n",
    "        w -= lr * grad(x, y, w, w0)\n",
    "    return w\n",
    "\n",
    "# predict with the updated weights\n",
    "def predict(x, w, w0):\n",
    "    y_pred = sigmoid(np.dot(x, w) + w0)\n",
    "    return y_pred\n",
    "\n",
    "# classification to binary\n",
    "def classify(y_pred):\n",
    "    y_pred_classes = np.round(y_pred)\n",
    "    return y_pred_classes\n",
    "\n",
    "for lr in learning_rates:\n",
    "    w_curr = copy.deepcopy(w).reshape(-1, 1)\n",
    "    w0_curr = copy.deepcopy(w0).reshape(-1, 1)\n",
    "    BatchGD(X_train.T, y_train.reshape(-1, 1), w_curr, w0_curr, lr, max_iter)\n",
    "    y_pred = predict(X_val.T, w_curr, w0_curr)\n",
    "    y_pred_classes = classify(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "    acc = np.mean(y_pred_classes == y_val)\n",
    "    train_accs.append(acc)\n",
    "    train_w.append(w_curr)\n",
    "    train_w0.append(w0_curr)\n",
    "\n",
    "best_acc = max(train_accs)\n",
    "best_lr = learning_rates[train_accs.index(best_acc)]\n",
    "\n",
    "print(\"Best learning rate:\", best_lr)\n",
    "print(\"Best accuracy on train:\", best_acc)\n",
    "print(\"Best Model weights:\\n\", train_w[train_accs.index(best_acc)])\n",
    "print(\"Best Model bias:\", train_w0[train_accs.index(best_acc)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab48f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with Sigmoid function\n",
    "# evaluate on test set\n",
    "w_eval = train_w[train_accs.index(best_acc)]\n",
    "w0_eval = train_w0[train_accs.index(best_acc)]\n",
    "y_pred = predict(X_test.T, w_eval, w0_eval)\n",
    "y_pred_classes = classify(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "acc = np.mean(y_pred_classes == y_test)\n",
    "print(\"Accuracy on test:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531376fe",
   "metadata": {},
   "source": [
    "(4) **[10 points]** Learning a simple perceptron with **SGD** (using the given initializations $w^{init}$ and $w^{init}_{0}$) based on the training set ($X_{train}$, $y_{train}$): use the training set and the validation set to obtain a good learning rate(you can set the maximum for iterations and try different learning rate); output the learned model and evaluate its performance on the test set with the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "67afd506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.0001\n",
      "Best accuracy on train: 0.97\n",
      "Best Model weights:\n",
      " [[-0.00041511]\n",
      " [-0.00058675]\n",
      " [-0.00043264]\n",
      " [-0.00053583]\n",
      " [-0.00058144]\n",
      " [-0.00067721]\n",
      " [-0.00079495]\n",
      " [-0.00056218]\n",
      " [-0.00081143]\n",
      " [-0.00064239]\n",
      " [-0.00021545]\n",
      " [-0.00044725]\n",
      " [-0.00042415]\n",
      " [-0.00037599]\n",
      " [-0.00043494]\n",
      " [-0.00060382]\n",
      " [-0.00074746]\n",
      " [-0.00050512]\n",
      " [-0.00037839]\n",
      " [-0.00047951]]\n",
      "Best Model bias: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# SGD Implementation\n",
    "import copy\n",
    "max_iter = 50\n",
    "learning_rates = [1e-4, 1e-8]\n",
    "\n",
    "train_accs = []\n",
    "train_w = []\n",
    "train_w0 = []\n",
    "best_lr = None\n",
    "best_acc = 0\n",
    "\n",
    "# compute the gradient of the loss function with respect to the weights\n",
    "def grad(x, y, w, w0):\n",
    "    y_pred = sigmoid(np.dot(x, w) + w0)\n",
    "    grad_w = np.dot(x.reshape((x.shape[0], 1)) , (y_pred - y)).reshape((x.shape[0], 1)) * 1 / 2\n",
    "    return grad_w\n",
    "\n",
    "# iterate and update the weights\n",
    "def StochasticGD(x, y, w, w0, lr, max_iter):\n",
    "    for i in range(max_iter):\n",
    "        index = np.random.randint(0, y.shape[0])\n",
    "        w -= lr * grad(x[index], y[index], w, w0)\n",
    "    return w\n",
    "\n",
    "# predict with the updated weights\n",
    "def predict(x, w, w0):\n",
    "    y_pred = sigmoid(np.dot(x, w) + w0)\n",
    "    return y_pred\n",
    "\n",
    "# classification to binary\n",
    "def classify(y_pred):\n",
    "    y_pred_classes = np.round(y_pred)\n",
    "    return y_pred_classes\n",
    "\n",
    "for lr in learning_rates:\n",
    "    w_curr = copy.deepcopy(w).reshape(-1, 1)\n",
    "    w0_curr = copy.deepcopy(w0).reshape(-1, 1)\n",
    "    StochasticGD(X_train.T, y_train.reshape(-1, 1), w_curr, w0_curr, lr, max_iter)\n",
    "    y_pred = predict(X_val.T, w_curr, w0_curr)\n",
    "    y_pred_classes = classify(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "    acc = np.mean(y_pred_classes == y_val)\n",
    "    train_accs.append(acc)\n",
    "    train_w.append(w_curr)\n",
    "    train_w0.append(w0_curr)\n",
    "\n",
    "best_acc = max(train_accs)\n",
    "best_lr = learning_rates[train_accs.index(best_acc)]\n",
    "\n",
    "print(\"Best learning rate:\", best_lr)\n",
    "print(\"Best accuracy on train:\", best_acc)\n",
    "print(\"Best Model weights:\\n\", train_w[train_accs.index(best_acc)])\n",
    "print(\"Best Model bias:\", train_w0[train_accs.index(best_acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "6830a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with Sigmoid function\n",
    "# evaluate on test set\n",
    "w_eval = train_w[train_accs.index(best_acc)]\n",
    "w0_eval = train_w0[train_accs.index(best_acc)]\n",
    "y_pred = predict(X_test.T, w_eval, w0_eval)\n",
    "y_pred_classes = classify(y_pred.reshape(-1, 1)).reshape(-1)\n",
    "acc = np.mean(y_pred_classes == y_test)\n",
    "print(\"Accuracy on test:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e691ce7",
   "metadata": {},
   "source": [
    "## (b) SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321dd481",
   "metadata": {},
   "source": [
    "(1) **[10 points]** Use the function **‘svm’** in package **‘sklearn’** to do the binary classification. Output the model and evaluate its performance on each dataset with the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "bc84bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.9864285714285714\n",
      "Accuracy on val: 0.98\n",
      "Accuracy on test: 0.965\n"
     ]
    }
   ],
   "source": [
    "# SVM Implementation\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train.T, y_train)\n",
    "y_train_pred = clf.predict(X_train.T)\n",
    "y_val_pred = clf.predict(X_val.T)\n",
    "y_test_pred = clf.predict(X_test.T)\n",
    "print(\"Accuracy on train:\", np.mean(y_train_pred == y_train))\n",
    "print(\"Accuracy on val:\", np.mean(y_val_pred == y_val))\n",
    "print(\"Accuracy on test:\", np.mean(y_test_pred == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd914a6",
   "metadata": {},
   "source": [
    "## (c) Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ae058",
   "metadata": {},
   "source": [
    "(1) **[4 points]** Try to compare  models learned from (a)(3), (a)(4) and (b). Write down your explanation and data support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebbffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "afce4ba17b5757b0751665fe068189bb8136ee183cc24f13fdedfef264cea2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
