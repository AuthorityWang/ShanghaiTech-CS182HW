{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.KNN estimator with a kernel funtion (20 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1Generate data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trian = np.concatenate([np.random.normal(-20, 10, 20), np.random.normal(10, 5, 10)])\n",
    "x_trian =x_trian.reshape((-1,1))\n",
    "x_test = np.linspace(np.min(x_trian),np.max(x_trian),100)\n",
    "x_test = x_test.reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sampling  many dpoins for approximating the true distribution\n",
    "# this is a multimodal density\n",
    "sns.kdeplot(np.concatenate([np.random.normal(-20, 10, 100), np.random.normal(10, 5, 100)]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Complete the code and draw the pictures\n",
    "Completing KNN estimator wtih a kernel funtion. \n",
    "The kernel funtion is a Gaussian kernel, defined by $$K(u)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{u^2}{\\tau}),$$\n",
    "where $\\tau$ is the length-scale. \n",
    "\n",
    "**Task:** You should complete the code for the KNN estimator with a Gaussian kernel function and plot the estimated density with three sets of parameters, as shown below\n",
    "- k=3,$\\tau=2$\n",
    "- k=3,$\\tau=0.2$\n",
    "- k=5,$\\tau=2$\n",
    "\n",
    "**Hint:** We have already generated a set of test points and saved them in the variable x_test. Your task is to plot the estimated density function $\\hat{p}(x)$ based on these points. The code framework has been provided, and you can either write it from scratch or fill in the missing parts in the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#Define your KNN estimator \n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "tau=2\n",
    "##################\n",
    "#Fill in the blanks by your code or using following template\n",
    "# kke = KNNKernelEstimator(k,tau)\n",
    "# kke.fit(x_trian)\n",
    "# predict_x = kke.predict(x_test)\n",
    "###################\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_test,predict_x)\n",
    "plt.title(\"k:\"+str(k)+r\"   $\\tau$:\"+str(tau),fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "k=3\n",
    "tau=0.2\n",
    "##################\n",
    "#Fill in the blanks by your code\n",
    "###################\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_test,predict_x)\n",
    "plt.title(\"k:\"+str(k)+r\"   $\\tau$:\"+str(tau),fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "k=5\n",
    "tau=2\n",
    "##################\n",
    "#Fill in the blanks by your code\n",
    "###################\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_test,predict_x)\n",
    "plt.title(\"k:\"+str(k)+r\"   $\\tau$:\"+str(tau),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Deep Learning for classification (30 pts)\n",
    "\n",
    "For this task, you are required to design and train a deep neural network to perform a classification task on a provided dataset. The dataset can be found at the following link: http://pan.shanghaitech.edu.cn/cloudservice/outerLink/decode?c3Vnb24xNjgyNzcwODk4OTU5c3Vnb24=\n",
    "\n",
    "The dataset consists of a training set and test set. The training set should be used to train your model, and the test set should be used to evaluate the performance of your model.\n",
    "\n",
    "**Your goal is to achieve at least 70% accuracy on the test set using your trained model. One point is deducted for every point the accuracy decreases**， such as 66.2% will lose 4 points.\n",
    "\n",
    "If your computer does not have the necessary resources to train a deep neural network, you may use the computing resources of a school computing cluster, Kaggle, or Google Colab.\n",
    "\n",
    "Hint:The use of pre-training models is prohibited (direct zero points) and custom neural networks are encouraged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# !pip install opencv-python  # the command for installing opencv,i.e, cv2\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import warnings\n",
    "import  torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set\n",
    "def readfile(path, label):\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        x[i, :, :] = cv2.resize(img,(128, 128))\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_dir = './food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "test_x, test_y = readfile(os.path.join(workspace_dir, \"testing\"), True)\n",
    "print(\"Size of test data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = transforms.Compose([\n",
    "            ######################\n",
    "            #Fill in the blanks by your code\n",
    "            #########################\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    ######################\n",
    "    #Fill in the blanks by your code\n",
    "    #########################\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = ImgDataset(train_x, train_y, train_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_set = ImgDataset(test_x, test_y, test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Construct deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#Define your nerual network\n",
    "##################################\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        ######################\n",
    "        #   Use the pytorch API to build the neural network\n",
    "        #########################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 trianing your model\n",
    "There are some tips which I hope can help you complete the task:\n",
    "- If the loss function goes down too slowly, you can make the step size larger. This trick is better combined with an adaptive learning rate regulator\n",
    "- If the model is overfitting, you can add a dropout layer in your model. \n",
    "-  [Data augmentation](https://pytorch.org/vision/stable/transforms.html) is also a good way to increase model generalization. We recommend that you do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier().cuda()\n",
    "loss = \n",
    "optimizer = \n",
    "num_epoch = \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_set.__len__(), train_loss/train_set.__len__()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model\n",
    "use your trained model to test the test set and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = \n",
    "        test_acc +=np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "\n",
    "test_acc = test_acc/test_set.__len__()\n",
    "print(\"Test Acc: \"+str(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e549ef825b32beaba8d70034b28e5b57a83caf526bd9e297c8551d2903cf287f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
